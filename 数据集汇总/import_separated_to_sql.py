# file: import_separated_to_sql.py
"""
åˆ†ç¦»æ•°æ®å¯¼å…¥SQL Serverï¼ˆç®€åŒ–ç‰ˆï¼‰
åŠŸèƒ½ï¼š
1. æŒ‰ä¾èµ–é¡ºåºå¯¼å…¥æ­Œæ›²ã€ç”¨æˆ·ã€äº¤äº’æ•°æ®
2. è‡ªåŠ¨è¿‡æ»¤å¤–é”®çº¦æŸå¤±è´¥çš„è®°å½•
3. é€‚é…çŸ­IDæ ¼å¼ï¼ˆU000001, S000001ç­‰ï¼‰
"""

import pandas as pd
import numpy as np
import os
from sqlalchemy import create_engine, text, inspect
import warnings
warnings.filterwarnings('ignore')

def clean_old_data(engine):
    """æŒ‰ä¾èµ–é¡ºåºæ¸…ç†æ—§æ•°æ®"""
    print("\n1. æ¸…ç†æ—§æ•°æ®...")
    
    delete_order = [
        "DELETE FROM comment_likes",
        "DELETE FROM song_comments",
        "DELETE FROM recommendations",
        "DELETE FROM user_song_interaction",
        "DELETE FROM test_interactions",
        "DELETE FROM train_interactions",
        "DELETE FROM filtered_interactions",
        "DELETE FROM song_id_mapping",
        "DELETE FROM enhanced_song_features",
        "DELETE FROM enhanced_user_features",
        "DELETE FROM algorithm_performance_stats",
        "DELETE FROM system_config"
    ]
    
    with engine.begin() as conn:
        for sql in delete_order:
            try:
                result = conn.execute(text(sql))
                print(f"   âœ… {sql:50} å½±å“è¡Œæ•°: {result.rowcount}")
            except Exception as e:
                if "doesn't exist" in str(e) or "å¯¹è±¡å" in str(e):
                    print(f"   âš ï¸ {sql:50} è¡¨ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                else:
                    print(f"   âŒ {sql:50} å¤±è´¥: {e}")
                    raise

def import_song_features(engine, data_dir):
    """å¯¼å…¥æ­Œæ›²ç‰¹å¾ï¼ˆç›´æ¥è¯»å–CSVå…¨éƒ¨åˆ—ï¼‰"""
    print("\n2. å¯¼å…¥æ­Œæ›²ç‰¹å¾...")
    song_file = os.path.join(data_dir, "all_song_features.csv")
    
    if not os.path.exists(song_file):
        print(f"   âŒ æ­Œæ›²æ–‡ä»¶ä¸å­˜åœ¨: {song_file}")
        return False
    
    song_df = pd.read_csv(song_file)
    print(f"   è¯»å–æ­Œæ›²æ•°æ®: {song_df.shape}")
    
    # ç›´æ¥ä½¿ç”¨CSVå…¨éƒ¨åˆ—ï¼ˆåˆ—åå·²ä¸æ•°æ®åº“ä¸€è‡´ï¼‰
    try:
        song_df.to_sql('enhanced_song_features', engine, if_exists='append', index=False)
        print(f"   âœ… æ­Œæ›²ç‰¹å¾å¯¼å…¥æˆåŠŸ: {len(song_df)} æ¡è®°å½•")
        return True
    except Exception as e:
        print(f"   âŒ æ­Œæ›²ç‰¹å¾å¯¼å…¥å¤±è´¥: {e}")
        return False

def import_user_features(engine, data_dir):
    """å¯¼å…¥ç”¨æˆ·ç‰¹å¾ï¼ˆåˆ†åˆ«å¤„ç†internal/externalï¼‰"""
    print("\n3. å¯¼å…¥ç”¨æˆ·ç‰¹å¾...")
    
    for source_type in ['internal', 'external']:
        user_file = os.path.join(data_dir, source_type, "user_features.csv")
        if os.path.exists(user_file):
            user_df = pd.read_csv(user_file)
            # ç¡®ä¿sourceå­—æ®µå­˜åœ¨ï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
            if 'source' not in user_df.columns:
                user_df['source'] = source_type
            # ç¡®ä¿roleå­—æ®µå­˜åœ¨
            if 'role' not in user_df.columns:
                user_df['role'] = 'user'
            try:
                user_df.to_sql('enhanced_user_features', engine, if_exists='append', index=False)
                print(f"   âœ… {source_type} ç”¨æˆ·ç‰¹å¾å¯¼å…¥æˆåŠŸ: {len(user_df)} æ¡")
            except Exception as e:
                print(f"   âŒ {source_type} ç”¨æˆ·ç‰¹å¾å¯¼å…¥å¤±è´¥: {e}")
        else:
            print(f"   âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {user_file}")

def import_interaction_data(engine, data_dir, source_type, table_name, file_name):
    """é€šç”¨å¯¼å…¥äº¤äº’æ•°æ®ï¼ˆè‡ªåŠ¨è·³è¿‡å¤–é”®çº¦æŸå¤±è´¥çš„è®°å½•ï¼‰"""
    file_path = os.path.join(data_dir, source_type, file_name)
    if not os.path.exists(file_path):
        print(f"   âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return
    
    df = pd.read_csv(file_path)
    print(f"   {source_type} {file_name}: {df.shape}")
    
    # è·å–æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„song_idå’Œuser_id
    with engine.connect() as conn:
        existing_songs = pd.read_sql("SELECT song_id FROM enhanced_song_features", conn)
        existing_users = pd.read_sql("SELECT user_id FROM enhanced_user_features", conn)
    
    valid_song_ids = set(existing_songs['song_id'].astype(str))
    valid_user_ids = set(existing_users['user_id'].astype(str))
    
    # ç¡®ä¿åˆ—ç±»å‹ä¸ºå­—ç¬¦ä¸²
    df['song_id'] = df['song_id'].astype(str)
    df['user_id'] = df['user_id'].astype(str)
    
    # è¿‡æ»¤æ— æ•ˆçš„å¤–é”®
    original_count = len(df)
    df = df[df['song_id'].isin(valid_song_ids) & df['user_id'].isin(valid_user_ids)]
    filtered_count = len(df)
    
    if filtered_count == 0:
        print(f"   âš ï¸ æ‰€æœ‰è®°å½•å‡å› å¤–é”®çº¦æŸè¢«è¿‡æ»¤ï¼Œè·³è¿‡å¯¼å…¥")
        return
    
    if filtered_count < original_count:
        print(f"   âš ï¸ è¿‡æ»¤äº† {original_count - filtered_count} æ¡æ— æ•ˆsong_id/user_idè®°å½•")
    
    # åˆ†æ‰¹å¯¼å…¥
    batch_size = 5000
    success_count = 0
    for i in range(0, len(df), batch_size):
        batch = df.iloc[i:i+batch_size]
        try:
            batch.to_sql(table_name, engine, if_exists='append', index=False)
            success_count += len(batch)
            print(f"     è¿›åº¦: {success_count}/{filtered_count}")
        except Exception as e:
            print(f"     æ‰¹æ¬¡å¯¼å…¥å¤±è´¥ï¼Œå°è¯•é€æ¡æ’å…¥...")
            for _, row in batch.iterrows():
                try:
                    row.to_frame().T.to_sql(table_name, engine, if_exists='append', index=False)
                    success_count += 1
                except:
                    continue
    
    print(f"   âœ… {source_type} {table_name} å¯¼å…¥æˆåŠŸ: {success_count}/{filtered_count} æ¡")

def import_separated_data_to_sql():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("åˆ†ç¦»æ•°æ®å¯¼å…¥SQL Serverï¼ˆç®€åŒ–ç‰ˆï¼‰")
    print("="*80)
    
    # æ•°æ®åº“é…ç½®ï¼ˆè¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
    db_config = {
        'server': 'localhost',
        'database': 'MusicRecommendationDB',
        'username': 'sa',
        'password': '123456',   # â† æ”¹æˆæ‚¨çš„å¯†ç 
        'driver': 'ODBC Driver 18 for SQL Server'
    }
    
    conn_str = (f"mssql+pyodbc://{db_config['username']}:{db_config['password']}"
                f"@{db_config['server']}/{db_config['database']}"
                f"?driver={db_config['driver'].replace(' ', '+')}&Encrypt=no")
    
    engine = create_engine(conn_str, echo=False)
    data_dir = "separated_processed_data"
    
    if not os.path.exists(data_dir):
        print(f"é”™è¯¯: æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return
    
    # 1. æ¸…ç†æ—§æ•°æ®
    clean_old_data(engine)
    
    # 2. å¯¼å…¥æ­Œæ›²ç‰¹å¾ï¼ˆå¿…é¡»æˆåŠŸï¼‰
    song_success = import_song_features(engine, data_dir)
    if not song_success:
        print("\nâŒ æ­Œæ›²ç‰¹å¾å¯¼å…¥å¤±è´¥ï¼Œç»ˆæ­¢åç»­å¯¼å…¥")
        return
    
    # 3. å¯¼å…¥ç”¨æˆ·ç‰¹å¾
    import_user_features(engine, data_dir)
    
    # 4. å¯¼å…¥äº¤äº’æ•°æ®ï¼ˆæŒ‰ä¾èµ–é¡ºåºï¼‰
    print("\n4. å¯¼å…¥äº¤äº’æ•°æ®...")
    # filtered_interactions
    import_interaction_data(engine, data_dir, 'internal', 'filtered_interactions', 'interaction_matrix.csv')
    import_interaction_data(engine, data_dir, 'external', 'filtered_interactions', 'interaction_matrix.csv')
    
    # 5. å¯¼å…¥è®­ç»ƒé›†
    print("\n5. å¯¼å…¥è®­ç»ƒé›†...")
    import_interaction_data(engine, data_dir, 'internal', 'train_interactions', 'train_interactions.csv')
    import_interaction_data(engine, data_dir, 'external', 'train_interactions', 'train_interactions.csv')
    
    # 6. å¯¼å…¥æµ‹è¯•é›†
    print("\n6. å¯¼å…¥æµ‹è¯•é›†...")
    import_interaction_data(engine, data_dir, 'internal', 'test_interactions', 'test_interactions.csv')
    import_interaction_data(engine, data_dir, 'external', 'test_interactions', 'test_interactions.csv')
    
    # 7. ç»Ÿè®¡éªŒè¯
    print("\n7. å¯¼å…¥ç»Ÿè®¡éªŒè¯...")
    try:
        with engine.connect() as conn:
            # æ­Œæ›²ç»Ÿè®¡
            song_stats = pd.read_sql("""
                SELECT source, COUNT(*) as count 
                FROM enhanced_song_features 
                GROUP BY source
            """, conn)
            print("\n   ğŸ“Š æ­Œæ›²ç»Ÿè®¡:")
            for _, row in song_stats.iterrows():
                print(f"     {row['source']}: {row['count']:,} é¦–")
            
            # ç”¨æˆ·ç»Ÿè®¡
            user_stats = pd.read_sql("""
                SELECT source, COUNT(*) as count 
                FROM enhanced_user_features 
                GROUP BY source
            """, conn)
            print("\n   ğŸ“Š ç”¨æˆ·ç»Ÿè®¡:")
            for _, row in user_stats.iterrows():
                print(f"     {row['source']}: {row['count']:,} ç”¨æˆ·")
            
            # äº¤äº’ç»Ÿè®¡ï¼ˆè¿‡æ»¤åï¼‰
            interaction_stats = pd.read_sql("""
                SELECT u.source, COUNT(*) as count 
                FROM filtered_interactions i
                JOIN enhanced_user_features u ON i.user_id = u.user_id
                GROUP BY u.source
            """, conn)
            print("\n   ğŸ“Š äº¤äº’ç»Ÿè®¡:")
            for _, row in interaction_stats.iterrows():
                print(f"     {row['source']}: {row['count']:,} äº¤äº’")
    except Exception as e:
        print(f"   âš ï¸ ç»Ÿè®¡éªŒè¯å¤±è´¥: {e}")
    
    print("\n" + "="*80)
    print("ğŸ‰ åˆ†ç¦»æ•°æ®å¯¼å…¥å®Œæˆï¼")
    print("="*80)

if __name__ == "__main__":
    import_separated_data_to_sql()